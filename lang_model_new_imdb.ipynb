{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ONE/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "# To create python file:\n",
    "# jupyter nbconvert --to=python lang_model_new.ipynb\n",
    "\n",
    "import utils.data_import as data_import\n",
    "import utils.imdb_data as imdb_data\n",
    "import utils.ml_utils as ml_utils\n",
    "import model.neural as neural\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data, vocab\n",
    "\n",
    "import os, sys\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "#from tqdm import tnrange, tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "import importlib\n",
    "\n",
    "import settings\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in notebook: True\n"
     ]
    }
   ],
   "source": [
    "in_notebook = ml_utils.in_ipynb()\n",
    "print('Running in notebook:', in_notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cuda.\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print('Cuda is available!')\n",
    "    print('Device:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print('No cuda.')\n",
    "\n",
    "if in_notebook:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing vocab from vocab.p... Done.\n",
      "Imported vocab:  10,954\n",
      "Read total of: 1,000 lines from imdb file.\n",
      "Number of classes: 2: {'negative': 0, 'positive': 1}\n",
      "Generated train: 700 lines\n",
      "Generated valid: 150 lines\n",
      "Generated test:  150 lines\n"
     ]
    }
   ],
   "source": [
    "corpus = imdb_data.ImdbCorpus(filename=settings.imdb_file, lines=settings.lines_imbd,\n",
    "                              vocab_file='vocab.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 47973), ('the', 13016), ('.', 10666), ('a', 6505), ('and', 6322), ('of', 5754), ('to', 5312), ('is', 4103), ('in', 3815), ('i', 3004), ('it', 2912), ('this', 2911), ('that', 2742), ('as', 1918), ('was', 1914), ('for', 1784), ('with', 1740), ('but', 1623), ('movie', 1489), ('film', 1378)]\n",
      "[('negative', 505), ('positive', 495)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus.vocab.most_frequent(to=20))\n",
    "print(corpus.classes.most_frequent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## sentiment: 0\n",
      "## text:      i firstly and completely and <unk> disagree with the <unk> who calls this a <unk> . <unk> <unk> is very serious about his film . he personally introduced the film at the <unk> i saw in chicago . he had worked on the film for years and it is the first in an intended <unk> . <unk> is <unk> is <unk> <unk> attempt at an art film in the vein of those he <unk> by <unk> <unk> etc <unk> i had heard rumor of this film years ago <unk> <unk> movie with all <unk> cast directed by <unk> <unk> . when it finally came out i watched the <unk> <unk> and read the <unk> and i was <unk> at the mouth with <unk> . . . <unk> went to chicago to see it and it was a major disappointment . if he took out the <unk> <unk> such as the <unk> <unk> and the dancing <unk> he would be left with something much <unk> but only about 10 minutes long <unk> in other words just watch the <unk> be <unk> and leave it at that . there are some striking images and fantastic <unk> and <unk> but its lack of focus amounts to disappointment . <eol> \n",
      "\n",
      "## sentiment: 0\n",
      "## text:      this 1919 to 1933 germany looks hardly like a post wwii czech <unk> . <unk> <unk> it is the czech <unk> and it is <unk> how <unk> <unk> this is one of the most <unk> history movies in the nearest past . <unk> is a head higher than adolf and looks so <unk> <unk> <unk> looks like 40 when he just is 23 and the <unk> always seems to look like 56 . and the <unk> <unk> even buildings have been <unk> sometimes . especially 1919 were a lot of houses in germany nearly new <unk> <unk> does not reach german <unk> . no <unk> <unk> then the <unk> . there have never been urban canyons around this <unk> never . and this may sound to you all like a <unk> in the year 1933 the greater berlin fire brigade <unk> a lot of vehicles with <unk> some even with <unk> <unk> but none with a hand <unk> <unk> one last <unk> what kind of <unk> castle was this at the final <unk> for me this was a kind of <unk> <unk> in <unk> <eol> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus.train.show_stoklist(corpus.vocab, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of batches: 35\n",
      " Preserved reviews: 700\n",
      " Matrix size:       torch.Size([2835, 20])\n"
     ]
    }
   ],
   "source": [
    "corpus.train.batchify(corpus.vocab, batch_size=settings.batch_size, \n",
    "                      seq_length=settings.window_size_imbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:        20\n",
      "Sequence length:   80\n",
      "Batchifying train...\n",
      " Number of batches: 35\n",
      " Preserved reviews: 700\n",
      " Matrix size:       torch.Size([2835, 20])\n",
      "Batchifying valid...\n",
      " Number of batches: 7\n",
      " Preserved reviews: 140\n",
      " Matrix size:       torch.Size([567, 20])\n",
      "Batchifying test... \n",
      " Number of batches: 7\n",
      " Preserved reviews: 140\n",
      " Matrix size:       torch.Size([567, 20])\n"
     ]
    }
   ],
   "source": [
    "corpus.batchify(batch_size=settings.batch_size, seq_length=settings.window_size_imbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 81],\n",
       " [81, 162],\n",
       " [162, 243],\n",
       " [243, 324],\n",
       " [324, 405],\n",
       " [405, 486],\n",
       " [486, 567],\n",
       " [567, 648],\n",
       " [648, 729],\n",
       " [729, 810],\n",
       " [810, 891],\n",
       " [891, 972],\n",
       " [972, 1053],\n",
       " [1053, 1134],\n",
       " [1134, 1215],\n",
       " [1215, 1296],\n",
       " [1296, 1377],\n",
       " [1377, 1458],\n",
       " [1458, 1539],\n",
       " [1539, 1620],\n",
       " [1620, 1701],\n",
       " [1701, 1782],\n",
       " [1782, 1863],\n",
       " [1863, 1944],\n",
       " [1944, 2025],\n",
       " [2025, 2106],\n",
       " [2106, 2187],\n",
       " [2187, 2268],\n",
       " [2268, 2349],\n",
       " [2349, 2430],\n",
       " [2430, 2511],\n",
       " [2511, 2592],\n",
       " [2592, 2673],\n",
       " [2673, 2754],\n",
       " [2754, 2835]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train.batch_start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## sentiment: 0\n",
      "## text:      [1249, 9389, 38, 1384, 38, 2, 5207, 119, 18, 2, 74, 6466, 456, 28, 2, 16, 2, 2, 27, 254, 868, 681, 342, 3936, 16, 472, 2950, 4355, 18, 3936, 275, 18, 2, 1249, 5149, 44, 1678, 16, 472, 503, 657, 93, 18, 3936, 40, 1110, 38, 47, 27, 18, 60, 44, 145, 1202, 2, 16, 2, 27, 2, 27, 2, 2, 3552, 275, 145, 853, 3936, 44, 18, 6631, 17, 245, 472, 2, 36, 2, 2, 1522, 2, 1249, 503, 680, 1209, 17, 456, 3936, 1110, 7692, 2, 2, 3922, 119, 564, 2, 973, 938, 36, 2, 2, 16, 215, 47, 1743, 1157, 292, 1249, 10055, 18, 2, 2, 38, 5545, 18, 2, 38, 1249, 130, 2, 275, 18, 2500, 119, 2, 16, 16, 16, 2, 3583, 23, 1678, 23, 4400, 47, 38, 47, 130, 28, 519, 4290, 16, 293, 472, 582, 292, 18, 2, 2, 103, 24, 18, 2, 2, 38, 18, 7044, 2, 472, 165, 203, 885, 119, 294, 688, 2, 160, 269, 681, 1042, 2741, 1267, 2, 44, 219, 2220, 552, 825, 18, 2, 203, 2, 38, 2323, 47, 275, 149, 16, 247, 79, 239, 7832, 4662, 38, 8351, 2, 38, 2, 160, 55, 3735, 17, 4647, 6460, 23, 4290, 16, 1]\n",
      "\n",
      "## sentiment: 0\n",
      "## text:      [456, 1871, 23, 2158, 7264, 3995, 10618, 185, 28, 474, 5826, 9466, 2, 16, 2, 2, 47, 27, 18, 9466, 2, 38, 47, 27, 2, 952, 2, 2, 456, 27, 216, 17, 18, 398, 2, 1012, 7429, 44, 18, 8623, 4824, 16, 2, 27, 28, 1288, 242, 244, 7870, 38, 3995, 470, 2, 2, 2, 3995, 185, 4098, 215, 472, 552, 27, 780, 38, 18, 2, 322, 8208, 23, 585, 185, 3193, 16, 38, 18, 2, 2, 1695, 1028, 333, 504, 2, 1954, 16, 5115, 1871, 626, 28, 5457, 17, 7027, 44, 7264, 1404, 549, 2, 2, 363, 158, 2595, 4401, 2, 16, 8, 2, 2, 572, 18, 2, 16, 247, 333, 1970, 504, 3237, 8603, 267, 456, 2, 1970, 16, 38, 456, 132, 3308, 23, 725, 564, 185, 28, 2, 44, 18, 150, 2158, 18, 2326, 10559, 555, 7016, 2, 28, 5457, 17, 966, 119, 2, 239, 1695, 119, 2, 2, 160, 569, 119, 28, 1293, 2, 2, 216, 546, 2, 588, 5531, 17, 2, 10562, 130, 456, 275, 18, 2300, 2, 40, 1241, 456, 130, 28, 5531, 17, 2, 2, 44, 2, 1]\n",
      "\n",
      "## sentiment: 0\n",
      "## text:      i firstly and completely and <unk> disagree with the <unk> who calls this a <unk> . <unk> <unk> is very serious about his film . he personally introduced the film at the <unk> i saw in chicago . he had worked on the film for years and it is the first in an intended <unk> . <unk> is <unk> is <unk> <unk> attempt at an art film in the vein of those he <unk> by <unk> <unk> etc <unk> i had heard rumor of this film years ago <unk> <unk> movie with all <unk> cast directed by <unk> <unk> . when it finally came out i watched the <unk> <unk> and read the <unk> and i was <unk> at the mouth with <unk> . . . <unk> went to chicago to see it and it was a major disappointment . if he took out the <unk> <unk> such as the <unk> <unk> and the dancing <unk> he would be left with something much <unk> but only about 10 minutes long <unk> in other words just watch the <unk> be <unk> and leave it at that . there are some striking images and fantastic <unk> and <unk> but its lack of focus amounts to disappointment . <eol> \n",
      "\n",
      "## sentiment: 0\n",
      "## text:      this 1919 to 1933 germany looks hardly like a post wwii czech <unk> . <unk> <unk> it is the czech <unk> and it is <unk> how <unk> <unk> this is one of the most <unk> history movies in the nearest past . <unk> is a head higher than adolf and looks so <unk> <unk> <unk> looks like 40 when he just is 23 and the <unk> always seems to look like 56 . and the <unk> <unk> even buildings have been <unk> sometimes . especially 1919 were a lot of houses in germany nearly new <unk> <unk> does not reach german <unk> . no <unk> <unk> then the <unk> . there have never been urban canyons around this <unk> never . and this may sound to you all like a <unk> in the year 1933 the greater berlin fire brigade <unk> a lot of vehicles with <unk> some even with <unk> <unk> but none with a hand <unk> <unk> one last <unk> what kind of <unk> castle was this at the final <unk> for me this was a kind of <unk> <unk> in <unk> <eol> \n",
      "\n",
      "tensor([1249, 9389,   38, 1384,   38,    2, 5207,  119,   18,    2,   74, 6466,\n",
      "         456,   28,    2,   16,    2,    2,   27,  254,  868,  681,  342, 3936,\n",
      "          16,  472, 2950, 4355,   18, 3936,  275,   18,    2, 1249, 5149,   44,\n",
      "        1678,   16,  472,  503,  657,   93,   18, 3936,   40, 1110,   38,   47,\n",
      "          27,   18,   60,   44,  145, 1202,    2,   16,    2,   27,    2,   27,\n",
      "           2,    2, 3552,  275,  145,  853, 3936,   44,   18, 6631,   17,  245,\n",
      "         472,    2,   36,    2,    2, 1522,    2, 1249])\n"
     ]
    }
   ],
   "source": [
    "if in_notebook:\n",
    "    corpus.train.show_itoklist(2)\n",
    "    corpus.train.show_stoklist(corpus.vocab, 2)\n",
    "    print(corpus.train.batch_matrix[:settings.window_size_imbd,0])\n",
    "    #df = corpus.train.batch_stats()\n",
    "    #df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = imdb_data.ImdbTextDataset(corpus.train)\n",
    "valid_dl = imdb_data.ImdbTextDataset(corpus.valid)\n",
    "test_dl = imdb_data.ImdbTextDataset(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 81], [81, 162], [162, 243], [243, 324], [324, 405]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train.batch_start_end[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1249, 4184,   44,  ..., 1249,    2,  456],\n",
       "         [9389,   23,   28,  ..., 3276,  130, 3922],\n",
       "         [  38,   55,  671,  ...,   23,  149, 9129],\n",
       "         ...,\n",
       "         [1522,   27, 2065,  ..., 2808, 1231, 1244],\n",
       "         [   2,   28,  725,  ...,  825,    2, 2067],\n",
       "         [1249, 3747,    2,  ...,   18, 7374,    2]]),\n",
       " tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 80\n",
      "contrary to its <unk> this film offers no <unk> and thus audience is subjected to a <unk> <unk> . all <unk> appears <unk> <unk> <unk> <unk> reminiscent of those <unk> plays available on <unk> including even the <unk> . everybody is <unk> shouting and doing odd things for no reason . the <unk> looks interesting as it is a straight lift from <unk> <unk> <unk> . john abraham who is so natural in almost all his films is a complete ## negative ##\n",
      "1 80\n",
      "<unk> of <unk> <unk> my bad <unk> of course only a movie starring <unk> simpson can include serious <unk> like this . . <unk> a norwegian and i felt <unk> and <unk> the makers of this movie did not take the time to do their research upon making this <unk> movie . even <unk> is more accurate when it comes to <unk> about this country <unk> so <unk> posting my <unk> out of my <unk> <unk> country is named <unk> ## negative ##\n",
      "2 80\n",
      "wonderful cast <unk> on <unk> script . ten or so adults <unk> at the summer camp they attended as juveniles . could this ever happen in a million <unk> <unk> simply a <unk> and a boring one at that . do they become <unk> <unk> do they <unk> their <unk> <unk> good <unk> they may try but ultimately the answer <unk> no . is there any <unk> any <unk> <unk> <unk> none of the above . how anyone can be ## negative ##\n",
      "3 80\n",
      "<unk> movie . very good photography of <unk> <unk> which seems now to be a different world . <unk> and an excellent <unk> and production . james mason a real first class star . it is and i would <unk> with the above comment that this movie is a national treasure . <eol> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> ## positive ##\n",
      "4 80\n",
      "the first von <unk> movie <unk> ever seen was breaking the waves . sure a <unk> movie but it <unk> stands in the shadow of europa . europa tells a story of a young <unk> who wants to experience germany just after the second world war . he takes a job that his uncle has arranged for him as a <unk> on a <unk> train . because of his <unk> he travels all through an almost totally destroyed <unk> meeting ## positive ##\n",
      "5 80\n",
      "this is seriously one of the best low <unk> b movies that i have ever seen . i am not one to stand up and <unk> during a <unk> but this one was <unk> worth it . obviously the <unk> is that there is a bed that <unk> <unk> well . . <unk> is a <unk> term i <unk> it really <unk> <unk> bubbles to <unk> the victims into itself and <unk> them in <unk> <unk> <unk> . the best ## positive ##\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(train_dl):\n",
    "    if idx > 5:\n",
    "        break\n",
    "    print(idx, len(x[:,1]))\n",
    "    for i in x[:,1]:\n",
    "        print(corpus.vocab.itos[i], end=' ')\n",
    "    print('##', corpus.classes.itos[y[1].item()], '##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural.class_model_LSTM(vocab_dim=len(corpus.vocab),\n",
    "                                emb_dim=settings.emb_dim,\n",
    "                                hidden_dim=settings.hidden_dim,\n",
    "                                n_layers=settings.num_layers,\n",
    "                                dropout=settings.dropout,\n",
    "                                n_classes=corpus.n_classes\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_model_LSTM(\n",
      "  (embedding): Embedding(10954, 50)\n",
      "  (lstm): LSTM(50, 300, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=600, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 20])\n",
      "torch.Size([20])\n",
      "torch.Size([20, 2])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[ 0.0217, -0.0017],\n",
      "        [-0.0211, -0.0044],\n",
      "        [ 0.0170,  0.0029],\n",
      "        [-0.0201,  0.0010],\n",
      "        [-0.0045,  0.0019],\n",
      "        [-0.0099,  0.0006],\n",
      "        [ 0.0031, -0.0085],\n",
      "        [-0.0003, -0.0038],\n",
      "        [ 0.0141,  0.0025],\n",
      "        [-0.0069, -0.0032],\n",
      "        [-0.0059, -0.0099],\n",
      "        [ 0.0072, -0.0012],\n",
      "        [-0.0079,  0.0057],\n",
      "        [-0.0145, -0.0085],\n",
      "        [-0.0159, -0.0169],\n",
      "        [-0.0124, -0.0074],\n",
      "        [ 0.0136,  0.0067],\n",
      "        [ 0.0081, -0.0019],\n",
      "        [ 0.0072,  0.0081],\n",
      "        [ 0.0068, -0.0018]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "x, y = train\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "preds = model(x)\n",
    "print(preds.shape)\n",
    "print(y)\n",
    "print(preds)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss = loss_func(preds, y.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('model_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclass = []\n",
    "missclass_next = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(model, train_dl, valid_dl, epochs=settings.epochs,\n",
    "               losses=[], missclass=[]):\n",
    "    \n",
    "    opt = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-6)\n",
    "    #opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "    #loss_func = nn.NLLLoss()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(opt, step_size=20, gamma=0.8)\n",
    "    \n",
    "    try: # Allow for user interrupt\n",
    " \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            scheduler.step()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            model.train() # turn on training mode\n",
    "\n",
    "            num_vals = 0\n",
    "            num_correct = 0\n",
    "\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            for x, y in tqdm(train_dl, desc='Train {}/{}'.format(epoch, epochs)):\n",
    "                opt.zero_grad()\n",
    "                \n",
    "                if cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "                preds = model(x)\n",
    "                loss = loss_func(preds, y.long())\n",
    "\n",
    "                loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "                opt.step()\n",
    "\n",
    "                running_loss += loss.item() * x.size(0) / x.size(1)\n",
    "\n",
    "                _, y_preds = torch.max(preds, dim=1)\n",
    "                num_correct += torch.sum(y == y_preds).item()\n",
    "                num_vals += y.size(0)\n",
    "\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            missclass_tr = 1 - num_correct / num_vals\n",
    "\n",
    "            epoch_loss = running_loss / len(train_dl)\n",
    "\n",
    "            num_vals = 0\n",
    "            num_correct = 0\n",
    "\n",
    "            # calculate the validation loss for this epoch\n",
    "            val_loss = 0.0\n",
    "            model.eval() # turn on evaluation mode\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in tqdm(valid_dl, desc='Valid {}/{}'.format(epoch, epochs)):\n",
    "                    if cuda:\n",
    "                        x = x.cuda()\n",
    "                        y = y.cuda()   \n",
    "                        \n",
    "                    preds = model(x)\n",
    "                    loss = loss_func(preds, y.long())\n",
    "\n",
    "                    val_loss += loss.item() * x.size(0) / x.size(1)\n",
    "\n",
    "                    _, y_preds = torch.max(preds, dim=1)\n",
    "                    num_correct += torch.sum(y == y_preds).item()\n",
    "                    num_vals += y.size(0)\n",
    "\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            missclass_te = 1 - num_correct / num_vals\n",
    "            val_loss /= len(valid_dl)\n",
    "            \n",
    "            missclass.append((missclass_tr, missclass_te))\n",
    "            losses.append((epoch_loss, val_loss))\n",
    "\n",
    "            print('Epoch: {}/{}, Loss: [{:.4f}, {:.4f}], Ppl: [{:6.2f}, {:6.2f}], '\n",
    "                  'Miss: [{:.2%}, {:.2%}]'\\\n",
    "                  .format(epoch, epochs, epoch_loss, val_loss, \n",
    "                          math.exp(epoch_loss), math.exp(val_loss), \n",
    "                          missclass_tr, missclass_te))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            print('Saving weights file...', end=' ', flush=True)\n",
    "            torch.save(model, 'model_weights_imdb.pt')\n",
    "            print('Done.', flush=True)\n",
    "            #to load: model = torch.load('model_weights.pt')\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print('Stopping with latest weights.')\n",
    "        \n",
    "    return model, opt, losses, missclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/10: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it]\n",
      "Valid 1/10: 100%|██████████| 7/7 [00:01<00:00,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Loss: [4.0447, 3.2823], Ppl: [ 57.09,  26.64], Miss: [49.14%, 46.43%]\n",
      "Saving weights file... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train 2/10: 100%|██████████| 35/35 [00:36<00:00,  1.04s/it]\n",
      "Valid 2/10: 100%|██████████| 7/7 [00:01<00:00,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10, Loss: [3.1273, 2.7967], Ppl: [ 22.81,  16.39], Miss: [49.29%, 47.14%]\n",
      "Saving weights file... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train 3/10: 100%|██████████| 35/35 [00:36<00:00,  1.05s/it]\n",
      "Valid 3/10: 100%|██████████| 7/7 [00:01<00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10, Loss: [3.0623, 2.8583], Ppl: [ 21.38,  17.43], Miss: [51.14%, 46.43%]\n",
      "Saving weights file... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train 4/10: 100%|██████████| 35/35 [00:36<00:00,  1.05s/it]\n",
      "Valid 4/10: 100%|██████████| 7/7 [00:01<00:00,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10, Loss: [2.9570, 2.8027], Ppl: [ 19.24,  16.49], Miss: [48.86%, 52.14%]\n",
      "Saving weights file... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train 5/10: 100%|██████████| 35/35 [00:38<00:00,  1.09s/it]\n",
      "Valid 5/10: 100%|██████████| 7/7 [00:01<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10, Loss: [2.8324, 2.9855], Ppl: [ 16.99,  19.80], Miss: [44.29%, 54.29%]\n",
      "Saving weights file... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train 6/10: 100%|██████████| 35/35 [00:37<00:00,  1.08s/it]\n",
      "Valid 6/10: 100%|██████████| 7/7 [00:01<00:00,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10, Loss: [2.9910, 2.9648], Ppl: [ 19.91,  19.39], Miss: [46.43%, 53.57%]\n",
      "Saving weights file... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train 7/10: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it]\n",
      "Valid 7/10: 100%|██████████| 7/7 [00:01<00:00,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10, Loss: [2.9099, 2.9403], Ppl: [ 18.36,  18.92], Miss: [46.71%, 56.43%]\n",
      "Saving weights file... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train 8/10: 100%|██████████| 35/35 [00:39<00:00,  1.12s/it]\n",
      "Valid 8/10: 100%|██████████| 7/7 [00:01<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10, Loss: [2.8977, 3.1777], Ppl: [ 18.13,  23.99], Miss: [48.57%, 54.29%]\n",
      "Saving weights file... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train 9/10:  89%|████████▊ | 31/35 [00:33<00:04,  1.08s/it]"
     ]
    }
   ],
   "source": [
    "model, opt, losses, missclass = run_epochs(model, train_dl, valid_dl, epochs=settings.epochs,\n",
    "                                           losses=losses, missclass=missclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(valid_dl))\n",
    "if cuda:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "preds = model(x)\n",
    "_, y_preds = torch.max(preds, dim=1)\n",
    "loss = loss_func(preds, y.long())\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "print(y[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_preds[:])\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_notebook:\n",
    "    plt.plot(losses)\n",
    "    plt.legend(['train', 'valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_notebook:\n",
    "    plt.plot(missclass)\n",
    "    plt.legend(['train', 'valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
