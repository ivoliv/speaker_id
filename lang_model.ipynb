{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ONE/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n",
      "/anaconda3/envs/ONE/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import utils.data_import as data_import\n",
    "import utils.ml_utils as ml_utils\n",
    "\n",
    "import torch, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data, vocab\n",
    "\n",
    "import os, sys\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "#from tqdm import tnrange, tqdm_notebook\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cuda.\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print('Cuda is available!')\n",
    "    print('Device:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print('No cuda.')\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length:  100,260,643 token sequence.\n",
      "Generated text length: 1,048,745,572 tokens.\n",
      "Writing files to ./data\n",
      "Train and validation files written to disk.\n",
      "Sizes: (1239449, 3) (218727, 3)\n"
     ]
    }
   ],
   "source": [
    "import settings\n",
    "\n",
    "if settings.ORIG_DATA == 0:\n",
    "    train_file = 'training.txt'\n",
    "    train, valid = data_import.normalize_and_split(org_data_path, train_file, test_size=settings.test_size)\n",
    "elif settings.ORIG_DATA == 1:\n",
    "    train_file = settings.imdb_file\n",
    "    train, valid = data_import.import_imbd(train_file, to=10000, test_size=settings.test_size)\n",
    "elif settings.ORIG_DATA == 2:\n",
    "    df = data_import.import_wikitext(settings.batch_size, window_size=settings.window_size, \n",
    "                                     lines=settings.lines, prob_cut=0.05, cut_factor=0.50,\n",
    "                                     corpus='wikitext-2')\n",
    "    train, valid = data_import.create_splits(df, test_size=settings.test_size)\n",
    "    \n",
    "data_import.create_split_files('.', train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= Valkyria Chronicles III =Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game\n",
      "Valkyria Chronicles III =Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1.239449e+06\n",
       "mean     6.772160e+01\n",
       "std      9.121875e+00\n",
       "min      1.500000e+01\n",
       "25%      6.500000e+01\n",
       "50%      6.900000e+01\n",
       "75%      7.300000e+01\n",
       "max      8.700000e+01\n",
       "Name: statement, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "statement_lengths = train_df['statement'].apply(lambda x: len(x.split()))\n",
    "print(train_df['statement'][0])\n",
    "print(train_df['tag'][0])\n",
    "display(statement_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>statement</th>\n",
       "      <th>tag_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1239449</th>\n",
       "      <td>the Sunday Mirror was critical of the song , g...</td>\n",
       "      <td>for the Sunday Mirror was critical of the song...</td>\n",
       "      <td>1239447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239450</th>\n",
       "      <td>Kylie clone \" . The single debuted at number n...</td>\n",
       "      <td>a Kylie clone \" . The single debuted at number...</td>\n",
       "      <td>1239448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239451</th>\n",
       "      <td>, which showed Ribbons wearing heavy makeup an...</td>\n",
       "      <td>filmed , which showed Ribbons wearing heavy ma...</td>\n",
       "      <td>1239449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239452</th>\n",
       "      <td>Then , Ribbons is shown seducing a man in the ...</td>\n",
       "      <td>. Then , Ribbons is shown seducing a man in th...</td>\n",
       "      <td>1239450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239453</th>\n",
       "      <td>performed on the Party in the Park event in 20...</td>\n",
       "      <td>first performed on the Party in the Park event...</td>\n",
       "      <td>1239451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       tag  \\\n",
       "1239449  the Sunday Mirror was critical of the song , g...   \n",
       "1239450  Kylie clone \" . The single debuted at number n...   \n",
       "1239451  , which showed Ribbons wearing heavy makeup an...   \n",
       "1239452  Then , Ribbons is shown seducing a man in the ...   \n",
       "1239453  performed on the Party in the Park event in 20...   \n",
       "\n",
       "                                                 statement   tag_id  \n",
       "1239449  for the Sunday Mirror was critical of the song...  1239447  \n",
       "1239450  a Kylie clone \" . The single debuted at number...  1239448  \n",
       "1239451  filmed , which showed Ribbons wearing heavy ma...  1239449  \n",
       "1239452  . Then , Ribbons is shown seducing a man in th...  1239450  \n",
       "1239453  first performed on the Party in the Park event...  1239451  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, lower=True)\n",
    "#LABEL = data.Field(sequential=True, use_vocab=True)\n",
    "#SENTIMENT = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = [('tag', TEXT),\n",
    "              ('statement', TEXT),\n",
    "              ('tag_id', None)]\n",
    "\n",
    "train, test = data.TabularDataset.splits(\n",
    "    path=data_path,\n",
    "    train='train.csv', validation='valid.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=datafields)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_file_imdb = settings.imdb_file\n",
    "train_imdb, valid_imdb = data_import.import_imbd(train_file_imdb, to=10000, test_size=settings.test_size)\n",
    "data_import.create_split_files('./imdb', train_imdb, valid_imdb)\n",
    "\n",
    "datafields = [('tag', None),\n",
    "              ('statement', TEXT),\n",
    "              ('tag_id', SENTIMENT)]\n",
    "\n",
    "train_imdb, test_imdb = data.TabularDataset.splits(\n",
    "    path='./imdb/data',\n",
    "    train='train.csv', validation='valid.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(train, test, train_imdb, test_imdb, vectors='glove.6B.'+str(settings.emb_dim)+'d')\n",
    "#TEXT.build_vocab(train, test, vectors='glove.6B.'+str(settings.emb_dim)+'d')\n",
    "TEXT.build_vocab(train, test)\n",
    "#LABEL.build_vocab(train, test, vectors='glove.6B.'+str(settings.emb_dim)+'d')\n",
    "#SENTIMENT.build_vocab(train_imdb, test_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_classes = len(LABEL.vocab)\n",
    "#print('Number of classes:', n_classes)\n",
    "#n_sent = len(dict(SENTIMENT.vocab.freqs).keys())\n",
    "#print('Number of sentiments:', n_sent)\n",
    "#print((TEXT.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(trn): 1239449\n",
      "len(test): 218727\n",
      "\n",
      "[('the', 12373681), (',', 9793663), ('.', 5766153), ('of', 5404532), ('and', 4937271), ('in', 4135368), ('to', 3926198), ('a', 3462903), ('\"', 2512106), ('was', 2120784), ('@-@', 1737459), ('=', 1686303), ('on', 1467423), (\"'s\", 1435488), ('as', 1408516), ('for', 1371161), ('that', 1360658), ('with', 1274651), ('by', 1231525), ('(', 1147989)]\n",
      "\n",
      "['<unk>', '<pad>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a']\n"
     ]
    }
   ],
   "source": [
    "print('len(trn):', len(train))\n",
    "print('len(test):', len(test))\n",
    "print()\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "print()\n",
    "print(TEXT.vocab.itos[:10])\n",
    "#print(vars(train[0]))\n",
    "#print()\n",
    "#print(LABEL.vocab.freqs.most_common(20))\n",
    "#print(LABEL.vocab.itos[:10])\n",
    "#print(LABEL.vocab.stoi)\n",
    "#print(SENTIMENT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 347414\n"
     ]
    }
   ],
   "source": [
    "#print(TEXT.vocab.vectors.shape)\n",
    "vocab_size = len(TEXT.vocab)\n",
    "print('Vocab size:',vocab_size)\n",
    "#TEXT.vocab.vectors[TEXT.vocab.stoi['the']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(trn): 1239449\n",
      "len(vld): 218727\n",
      "len(test): 218727\n"
     ]
    }
   ],
   "source": [
    "#trn, vld = train.split(0.7)\n",
    "trn = train\n",
    "vld = test\n",
    "print('len(trn):', len(trn))\n",
    "print('len(vld):', len(vld))\n",
    "print('len(test):', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = data.BucketIterator.splits(\n",
    "    datasets=(trn, vld),\n",
    "    batch_sizes=(settings.batch_size, settings.batch_size),\n",
    "    sort_key=lambda x: len(x.statement),\n",
    "    sort_within_batch=False,\n",
    "    repeat=False,\n",
    "    shuffle=False,\n",
    "    sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, dl, x_field, y_fields):\n",
    "        self.dl, self.x_field, self.y_fields = dl, x_field, y_fields\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x_field)\n",
    "            y = getattr(batch, self.y_fields)\n",
    "            #y = torch.cat([getattr(batch, feat).unsqueeze(1) \n",
    "            #               for feat in self.y_fields], dim=1).float()\n",
    "            if cuda:\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "            yield (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchGenerator(train_iter, 'statement', 'tag')\n",
    "valid_dl = BatchGenerator(val_iter, 'statement', 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleLSTM(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, n_layers=1, pretrained_vec=torch.zeros(0),\n",
    "                 change_emb=True, dropout=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        \n",
    "        self.embedding.weight.requires_grad = change_emb\n",
    "        \n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=n_layers,\n",
    "                            bidirectional=True, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(2*hidden_dim, len(TEXT.vocab))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.init_weights(pretrained_vec)\n",
    "        \n",
    "    def init_weights(self, pretrained_vec, initrange=0.1):\n",
    "        \n",
    "        if len(pretrained_vec) > 0:\n",
    "            print('Loaded pretrained vectors.')\n",
    "            self.embedding.weight.data.copy_(pretrained_vec)\n",
    "        else:\n",
    "            print('Not loaded pretrained vectors, uniform init in [-{}, {}].'.format(initrange, initrange))\n",
    "            self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "            \n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        # seq dims: [seq len, batch size]\n",
    "        \n",
    "        emb = self.embedding(seq)\n",
    "        # emb dims: [seq len, batch size, emb dim]\n",
    "        \n",
    "        out, (hid, cel) = self.lstm(emb)\n",
    "        \n",
    "        # out dims: [seq len, batch size, hidden_dim]\n",
    "        # hid dims: [2*n_layers, batch size, hidden_dim]\n",
    "        # cel dims: [2*n_layers, batch size, hidden_dim]\n",
    "        # out[-1,:,:hd] -> [batch size, hidden_dim]  (last time step hidden vector)\n",
    "        # out[0,:,hd:] <- [batch size, hidden_dim]  (first time step hidden vector)\n",
    "        # contatenation of last time period, whole batch, forward (first) chunck of hidden units\n",
    "        #   and the first time period, whole batch, backward (last) chunck of hidden units\n",
    "        #   (pytorch concatenates hidden units across dim #2 for bidirectional LSTM)\n",
    "        #conc = torch.cat((out[-1,:,:self.hidden_dim], out[0,:,self.hidden_dim:]), dim=1)\n",
    "        #conc = self.dropout(conc)\n",
    "        \n",
    "        output = self.dropout(out)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        # sm dims: [batch size, n_classes]\n",
    "        #sm = F.log_softmax(output, dim=-1)\n",
    "        #return sm\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not loaded pretrained vectors, uniform init in [-0.1, 0.1].\n"
     ]
    }
   ],
   "source": [
    "model = simpleLSTM(emb_dim=settings.emb_dim,\n",
    "                   hidden_dim=settings.hidden_dim,\n",
    "                   n_layers=settings.num_layers,\n",
    "                   #pretrained_vec=TEXT.vocab.vectors,\n",
    "                   change_emb=True,\n",
    "                   dropout=settings.dropout\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x, y = next(iter(train_dl))\n",
    "preds = model(x)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(preds.shape)\n",
    "print(x)\n",
    "print(y)\n",
    "print(preds.view(-1, preds.size(2)).shape)\n",
    "print(y.view(-1).shape)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss = loss_func(preds.view(-1, preds.size(2)), y.view(-1).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleLSTM(\n",
      "  (embedding): Embedding(347414, 300)\n",
      "  (lstm): LSTM(300, 1150, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=2300, out_features=347414, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = torch.load('model_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclass = []\n",
    "missclass_next = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def run_epochs(model, train_dl, valid_dl, epochs=settings.epochs,\n",
    "               losses=[], missclass=[]):\n",
    "    \n",
    "    opt = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-6)\n",
    "    #opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "    #loss_func = nn.NLLLoss()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(opt, step_size=20, gamma=0.8)\n",
    "    \n",
    "    try: # Allow for user interrupt\n",
    " \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            scheduler.step()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            model.train() # turn on training mode\n",
    "\n",
    "            num_vals = 0\n",
    "            num_correct = 0\n",
    "            miss_next_wd = 0\n",
    "            next_wd_tot = 0\n",
    "\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            for x, y in tqdm(train_dl, desc='Train {}/{}'.format(epoch, epochs)):\n",
    "                opt.zero_grad()\n",
    "\n",
    "                preds = model(x)\n",
    "                loss = loss_func(preds.view(-1, preds.size(2)), y.view(-1).long())\n",
    "\n",
    "                loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "                opt.step()\n",
    "\n",
    "                running_loss += loss.item() * x.size(0)\n",
    "\n",
    "                _, y_preds = torch.max(preds, dim=2)\n",
    "                num_correct += torch.sum(y == y_preds).item()\n",
    "                num_vals += y.size(0)*y.size(1)\n",
    "                \n",
    "                miss_next_wd += ml_utils.calc_miss_next_wds(y, y_preds)\n",
    "                next_wd_tot += y.size(1)\n",
    "\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            missclass_tr = 1 - num_correct / num_vals\n",
    "            miss_next_wd_rate_tr = miss_next_wd / next_wd_tot\n",
    "\n",
    "            epoch_loss = running_loss / len(trn)\n",
    "\n",
    "            num_vals = 0\n",
    "            num_correct = 0\n",
    "            miss_next_wd = 0\n",
    "            next_wd_tot = 0\n",
    "\n",
    "            # calculate the validation loss for this epoch\n",
    "            val_loss = 0.0\n",
    "            model.eval() # turn on evaluation mode\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in tqdm(valid_dl, desc='Valid {}/{}'.format(epoch, epochs)):\n",
    "                    preds = model(x)\n",
    "                    loss = loss_func(preds.view(-1, preds.size(2)), y.view(-1).long())\n",
    "\n",
    "                    val_loss += loss.item() * x.size(0)\n",
    "\n",
    "                    _, y_preds = torch.max(preds, dim=2)\n",
    "                    num_correct += torch.sum(y == y_preds).item()\n",
    "                    num_vals += y.size(0)*y.size(1)\n",
    "                    \n",
    "                    miss_next_wd += ml_utils.calc_miss_next_wds(y, y_preds)\n",
    "                    next_wd_tot += y.size(1)\n",
    "\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            missclass_te = 1 - num_correct / num_vals\n",
    "            val_loss /= len(vld)\n",
    "            \n",
    "            miss_next_wd_rate_val = miss_next_wd / next_wd_tot\n",
    "            \n",
    "            missclass_next.append((miss_next_wd_rate_tr, miss_next_wd_rate_val))\n",
    "            missclass.append((missclass_tr, missclass_te))\n",
    "            losses.append((epoch_loss, val_loss))\n",
    "\n",
    "            print('Epoch: {}/{}, Loss: [{:.4f}, {:.4f}], Ppl: [{:6.2f}, {:6.2f}], Miss: [{:.2%}, {:.2%}], [{:.2%}, {:.2%}]'\\\n",
    "                  .format(epoch, epochs, epoch_loss, val_loss, \n",
    "                          math.exp(epoch_loss), math.exp(val_loss), \n",
    "                          missclass_tr, missclass_te,\n",
    "                          miss_next_wd_rate_tr, miss_next_wd_rate_val))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            print('Saving weights file...', end=' ', flush=True)\n",
    "            torch.save(model, 'model_weights.pt')\n",
    "            print('Done.', flush=True)\n",
    "            #to load: model = torch.load('model_weights.pt')\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print('Stopping with latest weights.')\n",
    "        \n",
    "    return model, opt, losses, missclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/10:   0%|          | 0/15494 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model, opt, losses, missclass = run_epochs(model, train_dl, valid_dl, epochs=settings.epochs,\n",
    "                                           losses=losses, missclass=missclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(valid_dl))\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "preds = model(x)\n",
    "_, y_preds = torch.max(preds, dim=2)\n",
    "ml_utils.calc_miss_next_wds(y, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "y[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.legend(['train', 'valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(missclass)\n",
    "plt.legend(['train', 'valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(missclass_next)\n",
    "plt.legend(['train', 'valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Replace fc with sentiment layer\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, n_sent)\n",
    "if cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_imdb_iter, val_imdb_iter = data.BucketIterator.splits(\n",
    "    datasets=(train_imdb, test_imdb),\n",
    "    batch_sizes=(settings.batch_size, settings.batch_size),\n",
    "    sort_key=lambda x: len(x.statement),\n",
    "    sort_within_batch=False,\n",
    "    repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_imdb_dl = BatchGenerator(train_imdb_iter, 'statement', 'tag_id')\n",
    "valid_imdb_dl = BatchGenerator(val_imdb_iter, 'statement', 'tag_id')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model, opt, losses, missclass = run_epochs(model, train_imdb_dl, valid_imdb_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
