{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/r/ge.unx.sas.com/vol/vol620/u62/ivoliv/.conda/envs/ONE/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n",
      "/r/ge.unx.sas.com/vol/vol620/u62/ivoliv/.conda/envs/ONE/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import utils.data_import as data_import\n",
    "import utils.ml_utils as ml_utils\n",
    "\n",
    "import torch, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data, vocab\n",
    "\n",
    "import os, sys\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "#from tqdm import tnrange, tqdm_notebook\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available!\n",
      "Device: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print('Cuda is available!')\n",
    "    print('Device:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print('No cuda.')\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length:  2,051,910 token sequence.\n",
      "Generated text length: 21,042,720 tokens.\n",
      "Writing files to ./data\n",
      "Train and validation files written to disk.\n",
      "Sizes: (25354, 3) (4475, 3)\n"
     ]
    }
   ],
   "source": [
    "import settings\n",
    "\n",
    "if settings.ORIG_DATA == 0:\n",
    "    train_file = 'training.txt'\n",
    "    train, valid = data_import.normalize_and_split(org_data_path, train_file, test_size=settings.test_size)\n",
    "elif settings.ORIG_DATA == 1:\n",
    "    train_file = settings.imdb_file\n",
    "    train, valid = data_import.import_imbd(train_file, to=10000, test_size=settings.test_size)\n",
    "elif settings.ORIG_DATA == 2:\n",
    "    df = data_import.import_wikitext(window_size=settings.window_size, lines=settings.lines,\n",
    "                                     prob_cut=0.05, cut_factor=0.50)\n",
    "    train, valid = data_import.create_splits(df, test_size=settings.test_size)\n",
    "    \n",
    "data_import.create_split_files('.', train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50708\n",
      "the initial phase , Hill 355 would now not be secured until the afternoon of 4 October . The assault was being slowed by two positions on the northeast slopes of Hill 355 — known as Hill 220 — from which the Chinese held the British right flank in enfilade . C Company 3 RAR would be detached to assist the attack on Kowang @-@ San the next morning , with the Australians\n",
      "initial phase , Hill 355 would now not be secured until the afternoon of 4 October . The assault was being slowed by two positions on the northeast slopes of Hill 355 — known as Hill 220 — from which the Chinese held the British right flank in enfilade . C Company 3 RAR would be detached to assist the attack on Kowang @-@ San the next morning , with the Australians tasked\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    25354.000000\n",
       "mean        67.822553\n",
       "std          9.058326\n",
       "min         20.000000\n",
       "25%         66.000000\n",
       "50%         69.000000\n",
       "75%         73.000000\n",
       "max         89.000000\n",
       "Name: statement, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "statement_lengths = train_df['statement'].apply(lambda x: len(x.split()))\n",
    "print(train_df.shape[0] + train_df.shape[0])\n",
    "print(train_df['statement'][0])\n",
    "print(train_df['tag'][0])\n",
    "statement_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>statement</th>\n",
       "      <th>tag_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26012</th>\n",
       "      <td>. She knows how to renovate houses and manipul...</td>\n",
       "      <td>revolutionary . She knows how to renovate hous...</td>\n",
       "      <td>26012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13615</th>\n",
       "      <td>two victims were similar , though a survey of ...</td>\n",
       "      <td>the two victims were similar , though a survey...</td>\n",
       "      <td>13615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8939</th>\n",
       "      <td>leading , 10 – 7 . = = = Second quarter = = = ...</td>\n",
       "      <td>Tech leading , 10 – 7 . = = = Second quarter =...</td>\n",
       "      <td>8939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21467</th>\n",
       "      <td>to hate \" his villainous character . Critics p...</td>\n",
       "      <td>loved to hate \" his villainous character . Cri...</td>\n",
       "      <td>21467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9332</th>\n",
       "      <td>a protected area located in the Australian sta...</td>\n",
       "      <td>is a protected area located in the Australian ...</td>\n",
       "      <td>9332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tag  \\\n",
       "26012  . She knows how to renovate houses and manipul...   \n",
       "13615  two victims were similar , though a survey of ...   \n",
       "8939   leading , 10 – 7 . = = = Second quarter = = = ...   \n",
       "21467  to hate \" his villainous character . Critics p...   \n",
       "9332   a protected area located in the Australian sta...   \n",
       "\n",
       "                                               statement  tag_id  \n",
       "26012  revolutionary . She knows how to renovate hous...   26012  \n",
       "13615  the two victims were similar , though a survey...   13615  \n",
       "8939   Tech leading , 10 – 7 . = = = Second quarter =...    8939  \n",
       "21467  loved to hate \" his villainous character . Cri...   21467  \n",
       "9332   is a protected area located in the Australian ...    9332  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, lower=False)\n",
    "#LABEL = data.Field(sequential=True, use_vocab=True)\n",
    "#SENTIMENT = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = [('tag', TEXT),\n",
    "              ('statement', TEXT),\n",
    "              ('tag_id', None)]\n",
    "\n",
    "train, test = data.TabularDataset.splits(\n",
    "    path=data_path,\n",
    "    train='train.csv', validation='valid.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=datafields)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_file_imdb = settings.imdb_file\n",
    "train_imdb, valid_imdb = data_import.import_imbd(train_file_imdb, to=10000, test_size=settings.test_size)\n",
    "data_import.create_split_files('./imdb', train_imdb, valid_imdb)\n",
    "\n",
    "datafields = [('tag', None),\n",
    "              ('statement', TEXT),\n",
    "              ('tag_id', SENTIMENT)]\n",
    "\n",
    "train_imdb, test_imdb = data.TabularDataset.splits(\n",
    "    path='./imdb/data',\n",
    "    train='train.csv', validation='valid.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(train, test, train_imdb, test_imdb, vectors='glove.6B.'+str(settings.emb_dim)+'d')\n",
    "TEXT.build_vocab(train, test, vectors='glove.6B.'+str(settings.emb_dim)+'d')\n",
    "#TEXT.build_vocab(train, test)\n",
    "#LABEL.build_vocab(train, test, vectors='glove.6B.'+str(settings.emb_dim)+'d')\n",
    "#SENTIMENT.build_vocab(train_imdb, test_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 33277\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(dict(TEXT.vocab.freqs).keys())\n",
    "print('Vocab size:', n_vocab)\n",
    "#n_classes = len(dict(LABEL.vocab.freqs).keys())\n",
    "#print('Number of classes:', n_classes)\n",
    "#n_sent = len(dict(SENTIMENT.vocab.freqs).keys())\n",
    "#print('Number of sentiments:', n_sent)\n",
    "#print((TEXT.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(trn): 25354\n",
      "len(test): 4475\n",
      "\n",
      "[('the', 223038), (',', 196948), ('.', 144616), ('of', 112162), ('<unk>', 107641), ('and', 99768), ('in', 77775), ('to', 77204), ('a', 67424), ('=', 58263), ('\"', 55762), ('was', 41381), ('The', 34689), ('@-@', 33322), ('that', 27871), ('as', 27630), (\"'s\", 27611), ('on', 26911), ('for', 26260), ('with', 24881)]\n",
      "\n",
      "['<unk>', '<pad>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a']\n"
     ]
    }
   ],
   "source": [
    "print('len(trn):', len(train))\n",
    "print('len(test):', len(test))\n",
    "print()\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "print()\n",
    "print(TEXT.vocab.itos[:10])\n",
    "#print(vars(train[0]))\n",
    "#print()\n",
    "#print(LABEL.vocab.freqs.most_common(20))\n",
    "#print(LABEL.vocab.itos[:10])\n",
    "#print(LABEL.vocab.stoi)\n",
    "#print(SENTIMENT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(TEXT.vocab.vectors.shape)\n",
    "vocab_size = len(TEXT.vocab)\n",
    "#TEXT.vocab.vectors[TEXT.vocab.stoi['the']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(trn): 25354\n",
      "len(vld): 4475\n",
      "len(test): 4475\n"
     ]
    }
   ],
   "source": [
    "#trn, vld = train.split(0.7)\n",
    "trn = train\n",
    "vld = test\n",
    "print('len(trn):', len(trn))\n",
    "print('len(vld):', len(vld))\n",
    "print('len(test):', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = data.BucketIterator.splits(\n",
    "    datasets=(trn, vld),\n",
    "    batch_sizes=(settings.batch_size, settings.batch_size),\n",
    "    sort_key=lambda x: len(x.statement),\n",
    "    sort_within_batch=False,\n",
    "    repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, dl, x_field, y_fields):\n",
    "        self.dl, self.x_field, self.y_fields = dl, x_field, y_fields\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x_field)\n",
    "            y = getattr(batch, self.y_fields)\n",
    "            #y = torch.cat([getattr(batch, feat).unsqueeze(1) \n",
    "            #               for feat in self.y_fields], dim=1).float()\n",
    "            if cuda:\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "            yield (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchGenerator(train_iter, 'statement', 'tag')\n",
    "valid_dl = BatchGenerator(val_iter, 'statement', 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleLSTM(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, n_layers=1, pretrained_vec=torch.zeros(0),\n",
    "                 change_emb=True, dropout=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        \n",
    "        self.embedding.weight.requires_grad = change_emb\n",
    "        \n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=n_layers,\n",
    "                            bidirectional=True, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(2*hidden_dim, len(TEXT.vocab))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.init_weights(pretrained_vec)\n",
    "        \n",
    "    def init_weights(self, pretrained_vec, initrange=0.1):\n",
    "        \n",
    "        if len(pretrained_vec) > 0:\n",
    "            print('Loaded pretrained vectors.')\n",
    "            self.embedding.weight.data.copy_(pretrained_vec)\n",
    "        else:\n",
    "            print('Not loaded pretrained vectors, uniform init in [-{}, {}].'.format(initrange, initrange))\n",
    "            self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "            \n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        # seq dims: [seq len, batch size]\n",
    "        \n",
    "        emb = self.embedding(seq)\n",
    "        # emb dims: [seq len, batch size, emb dim]\n",
    "        \n",
    "        out, (hid, cel) = self.lstm(emb)\n",
    "        \n",
    "        # out dims: [seq len, batch size, hidden_dim]\n",
    "        # hid dims: [2*n_layers, batch size, hidden_dim]\n",
    "        # cel dims: [2*n_layers, batch size, hidden_dim]\n",
    "        # out[-1,:,:hd] -> [batch size, hidden_dim]  (last time step hidden vector)\n",
    "        # out[0,:,hd:] <- [batch size, hidden_dim]  (first time step hidden vector)\n",
    "        # contatenation of last time period, whole batch, forward (first) chunck of hidden units\n",
    "        #   and the first time period, whole batch, backward (last) chunck of hidden units\n",
    "        #   (pytorch concatenates hidden units across dim #2 for bidirectional LSTM)\n",
    "        #conc = torch.cat((out[-1,:,:self.hidden_dim], out[0,:,self.hidden_dim:]), dim=1)\n",
    "        #conc = self.dropout(conc)\n",
    "        \n",
    "        output = self.dropout(out)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        # sm dims: [batch size, n_classes]\n",
    "        #sm = F.log_softmax(output, dim=-1)\n",
    "        #return sm\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not loaded pretrained vectors, uniform init in [-0.1, 0.1].\n"
     ]
    }
   ],
   "source": [
    "model = simpleLSTM(emb_dim=settings.emb_dim,\n",
    "                   hidden_dim=settings.hidden_dim,\n",
    "                   n_layers=settings.num_layers,\n",
    "                   #pretrained_vec=TEXT.vocab.vectors,\n",
    "                   change_emb=True,\n",
    "                   dropout=settings.dropout\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x, y = next(iter(train_dl))\n",
    "preds = model(x)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(preds.shape)\n",
    "print(x)\n",
    "print(y)\n",
    "print(preds.view(-1, preds.size(2)).shape)\n",
    "print(y.view(-1).shape)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss = loss_func(preds.view(-1, preds.size(2)), y.view(-1).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vars(trn[0])['statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33278\n"
     ]
    }
   ],
   "source": [
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleLSTM(\n",
      "  (embedding): Embedding(33278, 300)\n",
      "  (lstm): LSTM(300, 1150, num_layers=3, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=2300, out_features=33278, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclass = []\n",
    "missclass_next = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_epochs(model, train_dl, valid_dl, epochs=settings.epochs,\n",
    "               losses=[], missclass=[]):\n",
    "    \n",
    "    opt = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-6)\n",
    "    #opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "    #loss_func = nn.NLLLoss()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(opt, step_size=20, gamma=0.8)\n",
    "    \n",
    "    try: # Allow for user interrupt\n",
    " \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            scheduler.step()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            model.train() # turn on training mode\n",
    "\n",
    "            num_vals = 0\n",
    "            num_correct = 0\n",
    "            miss_next_wd = 0\n",
    "            next_wd_tot = 0\n",
    "\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            for x, y in tqdm(train_dl, desc='Train {}/{}'.format(epoch, epochs)):\n",
    "                opt.zero_grad()\n",
    "\n",
    "                preds = model(x)\n",
    "                loss = loss_func(preds.view(-1, preds.size(2)), y.view(-1).long())\n",
    "\n",
    "                loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "                opt.step()\n",
    "\n",
    "                running_loss += loss.item() * x.size(0)\n",
    "\n",
    "                _, y_preds = torch.max(preds, dim=2)\n",
    "                num_correct += torch.sum(y == y_preds).item()\n",
    "                num_vals += y.size(0)*y.size(1)\n",
    "                \n",
    "                miss_next_wd += ml_utils.calc_miss_next_wds(y, y_preds)\n",
    "                next_wd_tot += y.size(1)\n",
    "\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            missclass_tr = 1 - num_correct / num_vals\n",
    "            miss_next_wd_rate_tr = miss_next_wd / next_wd_tot\n",
    "\n",
    "            epoch_loss = running_loss / len(trn)\n",
    "\n",
    "            num_vals = 0\n",
    "            num_correct = 0\n",
    "            miss_next_wd = 0\n",
    "            next_wd_tot = 0\n",
    "\n",
    "            # calculate the validation loss for this epoch\n",
    "            val_loss = 0.0\n",
    "            model.eval() # turn on evaluation mode\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in tqdm(valid_dl, desc='Valid {}/{}'.format(epoch, epochs)):\n",
    "                    preds = model(x)\n",
    "                    loss = loss_func(preds.view(-1, preds.size(2)), y.view(-1).long())\n",
    "\n",
    "                    val_loss += loss.item() * x.size(0)\n",
    "\n",
    "                    _, y_preds = torch.max(preds, dim=2)\n",
    "                    num_correct += torch.sum(y == y_preds).item()\n",
    "                    num_vals += y.size(0)*y.size(1)\n",
    "                    \n",
    "                    miss_next_wd += ml_utils.calc_miss_next_wds(y, y_preds)\n",
    "                    next_wd_tot += y.size(1)\n",
    "\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            missclass_te = 1 - num_correct / num_vals\n",
    "            val_loss /= len(vld)\n",
    "            \n",
    "            miss_next_wd_rate_val = miss_next_wd / next_wd_tot\n",
    "            \n",
    "            missclass_next.append((miss_next_wd_rate_tr, miss_next_wd_rate_val))\n",
    "            missclass.append((missclass_tr, missclass_te))\n",
    "            losses.append((epoch_loss, val_loss))\n",
    "\n",
    "            print('Epoch: {}/{}, Loss: [{:.4f}, {:.4f}], Ppl: [{:6.2f}, {:6.2f}], Miss: [{:.2%}, {:.2%}], [{:.2%}, {:.2%}]'\\\n",
    "                  .format(epoch, epochs, epoch_loss, val_loss, \n",
    "                          math.exp(epoch_loss), math.exp(val_loss), \n",
    "                          missclass_tr, missclass_te,\n",
    "                          miss_next_wd_rate_tr, miss_next_wd_rate_val))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            print('Saving weights file...', end=' ', flush=True)\n",
    "            torch.save(model, 'model_weights.pt')\n",
    "            print('Done.', flush=True)\n",
    "            #to load: model = torch.load('model_weights.pt')\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print('Stopping with latest weights.')\n",
    "        \n",
    "    return model, opt, losses, missclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a47d33389144fc697bb50eda36b1ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train 1/150', max=317), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/r/ge.unx.sas.com/vol/vol620/u62/ivoliv/.conda/envs/ONE/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b58b0426ee94e57bb98f744a0987824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Valid 1/150', max=56), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/150, Loss: [0.5028, 0.3310], Ppl: [  1.65,   1.39], Miss: [5.98%, 4.89%], [91.31%, 90.64%]\n",
      "Saving weights file... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/r/ge.unx.sas.com/vol/vol620/u62/ivoliv/.conda/envs/ONE/lib/python3.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type simpleLSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df1c577098c4569bad0c1f79808875e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train 2/150', max=317), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a3626f53e14df280d59a7607a9a1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Valid 2/150', max=56), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/150, Loss: [0.2935, 0.2363], Ppl: [  1.34,   1.27], Miss: [3.97%, 3.66%], [89.68%, 89.25%]\n",
      "Saving weights file... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74246f42fff4896a0cfa2b76709ee27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train 3/150', max=317), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, opt, losses, missclass = run_epochs(model, train_dl, valid_dl, epochs=settings.epochs,\n",
    "                                           losses=losses, missclass=missclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(valid_dl))\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "preds = model(x)\n",
    "_, y_preds = torch.max(preds, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size(0)*y.size(1)\n",
    "y[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.legend(['train', 'valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(missclass)\n",
    "plt.legend(['train', 'valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(missclass_next)\n",
    "plt.legend(['train', 'valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Replace fc with sentiment layer\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, n_sent)\n",
    "if cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_imdb_iter, val_imdb_iter = data.BucketIterator.splits(\n",
    "    datasets=(train_imdb, test_imdb),\n",
    "    batch_sizes=(settings.batch_size, settings.batch_size),\n",
    "    sort_key=lambda x: len(x.statement),\n",
    "    sort_within_batch=False,\n",
    "    repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_imdb_dl = BatchGenerator(train_imdb_iter, 'statement', 'tag_id')\n",
    "valid_imdb_dl = BatchGenerator(val_imdb_iter, 'statement', 'tag_id')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model, opt, losses, missclass = run_epochs(model, train_imdb_dl, valid_imdb_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
